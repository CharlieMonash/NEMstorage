{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efcdf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL FUNCTIONS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of unique colors for the lines\n",
    "unique_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b',\n",
    "                 '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#aaffc3', '#ffaec9',\n",
    "                 '#a9a9a9', '#ffd700', '#4682b4']\n",
    "\n",
    "# Create a custom color cycle\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=unique_colors)\n",
    "\n",
    "def queryServer(regions, new_data):\n",
    "    \"\"\"\n",
    "    Return all data from the server in 6 dataframes (df_all, TAS, SA, QLD, NSW, VIC)\n",
    "    \n",
    "    If IP blocked: Azure> aemo-server sql > (security) > networking > (firewall rules) > add IP\n",
    "    \"\"\"\n",
    "    # Connect to Server\n",
    "    if new_data == False and os.path.exists(\"TAS_Data.csv\") and os.path.exists(\"SA_Data.csv\") and os.path.exists(\"QLD_Data.csv\") and os.path.exists(\"NSW_Data.csv\") and os.path.exists(\"VIC_Data.csv\"):\n",
    "        df_all =  pd.DataFrame()\n",
    "        TAS =  pd.DataFrame()\n",
    "        SA =  pd.DataFrame()\n",
    "        QLD =  pd.DataFrame()\n",
    "        NSW =  pd.DataFrame()\n",
    "        VIC =  pd.DataFrame()\n",
    "        return df_all, TAS, SA, QLD, NSW, VIC\n",
    "    else:\n",
    "        connection_string = (\"ENTER YOUR CONNECTION STRING HERE\")\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "\n",
    "        # Call Data from Server\n",
    "        query_all = 'SELECT * FROM AEMO_Price_Demand_Data'\n",
    "        data_all = pd.read_sql(query_all, connection)\n",
    "        data_all['SETTLEMENTDATE'] = pd.to_datetime(data_all['SETTLEMENTDATE']) # changes datetime format\n",
    "        df_all = data_all.drop_duplicates(subset=['SETTLEMENTDATE', 'REGION'], keep='first') # deletes any duplicates\n",
    "\n",
    "        # Seperate data frame by state\n",
    "        TAS = df_all[df_all[\"REGION\"] == 'TAS1']\n",
    "        SA = df_all[df_all[\"REGION\"] == 'SA1']\n",
    "        QLD = df_all[df_all[\"REGION\"] == 'QLD1']\n",
    "        NSW = df_all[df_all[\"REGION\"] == 'NSW1']\n",
    "        VIC = df_all[df_all[\"REGION\"] == 'VIC1']\n",
    "\n",
    "        return df_all, TAS, SA, QLD, NSW, VIC\n",
    "\n",
    "\n",
    "def timeFormatting(regions, new_data = False, check = True):\n",
    "    \"\"\"\n",
    "    Resample df to make all data intervals 30 minutes\n",
    "    Where data is missing (ie. intevals > 30 min), rows are created for the missing 30 min interval and demand = 0 and RRP = NaN (0)\n",
    "    EXTREMELY SLOW TO format data, thus download and reupload csv, which will miss recent data\n",
    "    In future could do it so that it only does recent data and runs every day etc.\n",
    "    INPUTS:\n",
    "        regions - list of regions\n",
    "        new_data - Boolean check for if to append new data to existing data, if existing data exists\n",
    "        check = True - checks that the function has worked properly by printing values of the output df where interval != 30\n",
    "    OUTPUTS:\n",
    "        global variables for each reach with data formatted to 30 minute intervals\n",
    "    \"\"\"\n",
    "\n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    # Select Region\n",
    "    for df_name in regions:\n",
    "        file_path = df_name + \"_Data.csv\"\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            \n",
    "            print(f'{df_name}: resampled data exists. Calling from csv.')\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "            \n",
    "            # Get the last date in the existing CSV\n",
    "            last_date_in_csv = df['SETTLEMENTDATE'].max()\n",
    "            print(f\"     CSV Last Date: {last_date_in_csv}\")\n",
    "          \n",
    "            if new_data == True:\n",
    "                # Get the data from temp_data after the last date in CSV\n",
    "                temp_data = globals()[df_name]\n",
    "                temp_data['SETTLEMENTDATE'] = pd.to_datetime(temp_data['SETTLEMENTDATE'])\n",
    "\n",
    "                last_date = temp_data['SETTLEMENTDATE'].max()\n",
    "                print(f\"     SQL Last Date: {last_date}\")\n",
    "\n",
    "                if last_date != last_date_in_csv:\n",
    "                    temp_data = temp_data[temp_data['SETTLEMENTDATE'] > last_date_in_csv]\n",
    "                    temp_data.set_index('SETTLEMENTDATE', inplace=True)\n",
    "\n",
    "\n",
    "                    # Define custom aggregation functions\n",
    "                    sum_agg = lambda x: x.sum()  # For 'TOTALDEMAND'\n",
    "                    weighted_avg_agg = lambda x: (np.sum(x * temp_data.loc[x.index, 'TOTALDEMAND'])) / np.sum(temp_data.loc[x.index, 'TOTALDEMAND'])  # For 'RRP'\n",
    "\n",
    "                    # Resample 'TOTALDEMAND' using sum aggregation and 'RRP' using weighted average aggregation to 30 minute interval\n",
    "                    resampled_df = temp_data.resample('30T', label='left', closed='left').agg({'TOTALDEMAND': sum_agg, 'RRP': weighted_avg_agg})\n",
    "                    resampled_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "                    # Remove last interval as to ensure consistent time periods\n",
    "                    resampled_df = resampled_df.iloc[:-1]\n",
    "                    resampled_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "            \n",
    "            globals()[df_name] = pd.read_csv(file_path)\n",
    "            \n",
    "            if check == True:\n",
    "                # Check time intervals\n",
    "                # Calculate the intervals in hours\n",
    "                resampled_df = globals()[df_name]\n",
    "                intervals = resampled_df['SETTLEMENTDATE'].diff() / pd.Timedelta(hours=1)\n",
    "\n",
    "                # Check if any interval is not equal to 0.5 hours\n",
    "                not_equal_to_half = intervals != 0.5\n",
    "\n",
    "                # Print the rows where the interval is not equal to 0.5 hours\n",
    "                print('Below sample not at 30 minute interval:')\n",
    "                print(resampled_df[not_equal_to_half])\n",
    "                print(\"\")\n",
    "\n",
    "        else:\n",
    "            print(f'{df_name}: Resampling data to 30 min intervals, overwriting original data frames and saving csv files')\n",
    "            # Change index to date\n",
    "            temp_data = globals()[df_name]\n",
    "            temp_data.set_index('SETTLEMENTDATE', inplace=True)\n",
    "\n",
    "            # Define custom aggregation functions\n",
    "            sum_agg = lambda x: x.sum()  # For 'TOTALDEMAND'\n",
    "            weighted_avg_agg = lambda x: (np.sum(x * temp_data.loc[x.index, 'TOTALDEMAND'])) / np.sum(temp_data.loc[x.index, 'TOTALDEMAND'])  # For 'RRP'\n",
    "\n",
    "            # Resample 'TOTALDEMAND' using sum aggregation and 'RRP' using weighted average aggregation to 30 minute interval\n",
    "            resampled_df = temp_data.resample('30T', label='left', closed='left').agg({'TOTALDEMAND': sum_agg, 'RRP': weighted_avg_agg})\n",
    "            resampled_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "            # Remove first and last interval as to ensure consistent time periods\n",
    "            resampled_df = resampled_df.iloc[1:-1]\n",
    "\n",
    "            resampled_df.to_csv(file_path, index = False)\n",
    "            print(f\"Location: {df_name} saved as {output_file}\")\n",
    "\n",
    "            if check == True:\n",
    "                # Check time intervals\n",
    "                # Calculate the intervals in hours\n",
    "                intervals = resampled_df['SETTLEMENTDATE'].diff() / pd.Timedelta(hours=1)\n",
    "\n",
    "                # Check if any interval is not equal to 0.5 hours\n",
    "                not_equal_to_half = intervals != 0.5\n",
    "\n",
    "                # Print the rows where the interval is not equal to 0.5 hours\n",
    "                print('Below sample not at 30 minute interval:')\n",
    "                print(resampled_df[not_equal_to_half])\n",
    "                print(\"\")\n",
    "\n",
    "             # Overwrite the original DataFrame with the modified DataFrame\n",
    "            globals()[df_name] = resampled_df\n",
    "\n",
    "    return TAS, SA, QLD, NSW, VIC\n",
    "\n",
    "\n",
    "def combineRegions(regions):\n",
    "    \"\"\"\n",
    "    Weighted average of dataset for entire NEM\n",
    "    \"\"\"\n",
    "    data = pd.concat([VIC, NSW, SA, QLD, TAS], ignore_index=True)\n",
    "\n",
    "    # Calculate total demand for each time step\n",
    "    total_demand = data.groupby('SETTLEMENTDATE')['TOTALDEMAND'].sum()\n",
    "\n",
    "    # Calculate the weighted average of prices using demand as weights\n",
    "    weighted_price = (data['TOTALDEMAND'] * data['RRP']).groupby(data['SETTLEMENTDATE']).sum() / total_demand\n",
    "\n",
    "    # Create a new DataFrame to store the combined values\n",
    "    combined_data = pd.DataFrame({'TOTALDEMAND': total_demand, 'RRP': weighted_price})\n",
    "    combined_data = combined_data.reset_index()\n",
    "    combined_data.to_csv('AUS_Data.csv', index = False)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def dayMonthRevenue(regions, rte, new_data):\n",
    "    \"\"\"\n",
    "    Calculate daily revenue and average monthly revenue, and save as csv OR upload csv containing this data\n",
    "    INPUTS:\n",
    "        regions - list of regions\n",
    "        rte - round trip efficiency of storage device being examined\n",
    "        check = True - checks that the function has worked properly by printing values of the output df where interval != 30\n",
    "    OUTPUTS:\n",
    "        global variables for each reach with data formatted to 30 minute intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Select Region\n",
    "    for df_name in regions:\n",
    "        file_path1 = df_name + \"_\" + str(rte) + \"_DailyRevenue.csv\"\n",
    "        file_path2 = df_name + \"_\" + str(rte) + \"_AvMonthlyRevenue.csv\"\n",
    "\n",
    "        if os.path.exists(file_path1) and os.path.exists(file_path2) and new_data == False:\n",
    "            # Import Normalised Data\n",
    "            dr_name = df_name + '_DR'\n",
    "            aMR_name = df_name + '_aMR'\n",
    "\n",
    "            globals()[dr_name] = pd.read_csv(file_path1)\n",
    "            globals()[dr_name]['date'] = pd.to_datetime(globals()[dr_name]['date'])\n",
    "            globals()[aMR_name] = pd.read_csv(file_path2)\n",
    "\n",
    "        else:\n",
    "            df = globals()[df_name]\n",
    "            temp_data = df.copy()\n",
    "\n",
    "            # Group the DataFrame by the 'date' column\n",
    "            temp_data['SETTLEMENTDATE'] = pd.to_datetime(temp_data['SETTLEMENTDATE'])\n",
    "            grouped_df = temp_data.groupby(temp_data['SETTLEMENTDATE'].dt.date)\n",
    "\n",
    "            # Create a new DataFrame to store the results\n",
    "            dailyRevenue_df = pd.DataFrame(columns=['date'] + [f'hours={n/2}' for n in range(1, 25)])\n",
    "\n",
    "            # Iterate over each group\n",
    "            for date, group in grouped_df:\n",
    "                price_differences = {}  # Dictionary to store price differences for different values of 'n'\n",
    "                for n in range(1, 25):\n",
    "                    # Calculate the difference between the two highest and two lowest prices\n",
    "                    price_difference = group.nlargest(n, 'RRP')['RRP'].sum() * rte - group.nsmallest(n, 'RRP')['RRP'].sum()\n",
    "\n",
    "                    # Store the price difference in the dictionary\n",
    "                    price_differences[f'hours={n/2}'] = price_difference*0.5 # as each interval is half an hour\n",
    "\n",
    "                # Append the results to the result DataFrame\n",
    "                dailyRevenue_df = dailyRevenue_df.append({'date': date, **price_differences}, ignore_index=True)\n",
    "\n",
    "            # Convert the 'settlementdate' column to datetime if it's not already in datetime format\n",
    "            dailyRevenue_df['date'] = pd.to_datetime(dailyRevenue_df['date'])\n",
    "\n",
    "            # Create a new column for the month and Group the DataFrame by the month\n",
    "            dailyRevenue_df['month'] = dailyRevenue_df['date'].dt.to_period('M')\n",
    "            monthlyAvRevenue_df = dailyRevenue_df.groupby('month')\n",
    "\n",
    "            # Calculate the average for each column\n",
    "            avgRevMonth = monthlyAvRevenue_df.mean()\n",
    "            dailyRevenue_df = dailyRevenue_df.drop('month', axis=1)\n",
    "\n",
    "            dailyRevenue_df.to_csv(file_path1, index = False)\n",
    "            avgRevMonth.to_csv(file_path2, index = True)\n",
    "            print(f\"Location: {df_name}\")\n",
    "\n",
    "            # Overwrite the original DataFrame with the modified DataFrame\n",
    "            globals()[df_name + '_DR'] = dailyRevenue_df\n",
    "            globals()[df_name + '_aMR'] = avgRevMonth\n",
    "            \n",
    "    return TAS_DR, SA_DR, QLD_DR, NSW_DR, VIC_DR, AUS_DR, TAS_aMR, SA_aMR, QLD_aMR, NSW_aMR, VIC_aMR, AUS_aMR       \n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "\n",
    "    values, weights -- NumPy ndarrays with the same shape.\n",
    "    \"\"\"\n",
    "    average = numpy.average(values, weights=weights)\n",
    "    # Fast and numerically precise:\n",
    "    variance = numpy.average((values-average)**2, weights=weights)\n",
    "    return (average, math.sqrt(variance))\n",
    "\n",
    "\n",
    "def yearlyRevenuePlot(regions, financial_year, years):\n",
    "    \"\"\"\n",
    "    Plot yearly total revenue according to number of hours of energy storage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the year from the settlementdate column\n",
    "    for i, df_name in enumerate(regions):\n",
    "        name = df_name + '_DR'\n",
    "        df = globals()[name]\n",
    "        dailyRevenue = df.copy()\n",
    "\n",
    "        # Drop half hour increments of number of hours of storage\n",
    "        drop_idx = list(range(1,dailyRevenue.shape[1],2)) #Indexes to drop\n",
    "        drop_cols = [j for i,j in enumerate(dailyRevenue.columns) if i in drop_idx] #<--\n",
    "        dailyRevenue_df = dailyRevenue.drop(drop_cols, axis=1)         #Drop them at once over axis=1\n",
    "        \n",
    "        # Rename columns by keeping only the numeric part\n",
    "        dailyRevenue_df.columns = [col.split('=')[1] if 'hours=' in col else col for col in dailyRevenue_df.columns]\n",
    "        \n",
    "        if financial_year == True:\n",
    "            # Define the start and end months of your financial year\n",
    "            financial_year_start_month = 7  # July\n",
    "            financial_year_end_month = 6    # June\n",
    "            \n",
    "            ## Calculate the financial year based on the date column\n",
    "            dailyRevenue_df['year'] = (dailyRevenue_df['date'].dt.year - (dailyRevenue_df['date'].dt.month < financial_year_start_month))\n",
    "\n",
    "            # Calculate the financial year based on the date column using the defined function\n",
    "            #dailyRevenue_df['year'] = dailyRevenue_df.apply(calculate_financial_year, axis=1)\n",
    "            x_label = 'Financial Year'\n",
    "\n",
    "\n",
    "        else:\n",
    "            dailyRevenue_df['year'] = dailyRevenue_df['date'].dt.year\n",
    "            x_label = 'Calendar Year'\n",
    "\n",
    "        # Group the DataFrame by year and n, and calculate the sum of revenue\n",
    "        yearly_total_revenue = dailyRevenue_df.groupby('year').mean()\n",
    "\n",
    "        # Calculate the marginal increase in revenue for each extra hour of storage\n",
    "        \n",
    "        # Drop incomplete year and unnecessary columns\n",
    "        dailyRevenue_df = dailyRevenue_df.drop('year', axis=1)\n",
    "        yearly_total_revenue.drop(yearly_total_revenue.tail(1).index,inplace=True)\n",
    "\n",
    "        yearly_total_revenue.tail(years).plot.bar(figsize=(10, 6))\n",
    "        y_label = 'Average Daily Revenue (A$/MW)' #? $AU / kW-year\n",
    " \n",
    "        # Set plot labels and title\n",
    "        plt_title = 'Value of Potential Revenue from Storage for ' + df_name\n",
    "        #plt_title = 'Value of Potential Storage Revenue for the NEM'\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(plt_title)\n",
    "        plt.legend(title='Hours of Storage', ncol=2)\n",
    "        plt.grid(True)\n",
    "        plt.show()  \n",
    "        \n",
    "        globals()[df_name + '_YtR'] = yearly_total_revenue\n",
    "      # globals()[df_name + '_MIR'] = marginal_revenue_increase\n",
    "\n",
    "        \n",
    "def yearlyProfitPlot(regions, financial_year, years, lcos):\n",
    "    \"\"\"\n",
    "    Plot yearly total revenue according to number of hours of energy storage\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the year from the settlementdate column\n",
    "    for i, df_name in enumerate(regions):\n",
    "        name = df_name + '_DR'\n",
    "        df = globals()[name]\n",
    "        dailyRevenue = df.copy()\n",
    "        for i, column in enumerate(dailyRevenue.columns[1:], start=2):\n",
    "            dailyRevenue[column] = dailyRevenue[column] - (i-1) * (lcos/2)\n",
    "        globals()[df_name + '_DP'] = dailyRevenue\n",
    "\n",
    "        # Drop half hour increments of number of hours of storage\n",
    "        drop_idx = list(range(1,dailyRevenue.shape[1],2)) #Indexes to drop\n",
    "        drop_cols = [j for i,j in enumerate(dailyRevenue.columns) if i in drop_idx] #<--\n",
    "        dailyRevenue_df = dailyRevenue.drop(drop_cols, axis=1)         #Drop them at once over axis=1\n",
    "        \n",
    "        # Rename columns by keeping only the numeric part\n",
    "        dailyRevenue_df.columns = [col.split('=')[1] if 'hours=' in col else col for col in dailyRevenue_df.columns]\n",
    "        \n",
    "        if financial_year == True:\n",
    "            # Define the start and end months of your financial year\n",
    "            financial_year_start_month = 7  # July\n",
    "            financial_year_end_month = 6    # June\n",
    "            \n",
    "            ## Calculate the financial year based on the date column\n",
    "            dailyRevenue_df['year'] = (dailyRevenue_df['date'].dt.year - (dailyRevenue_df['date'].dt.month < financial_year_start_month))\n",
    "\n",
    "            # Calculate the financial year based on the date column using the defined function\n",
    "            #dailyRevenue_df['year'] = dailyRevenue_df.apply(calculate_financial_year, axis=1)\n",
    "            x_label = 'Financial Year'\n",
    "\n",
    "\n",
    "        else:\n",
    "            dailyRevenue_df['year'] = dailyRevenue_df['date'].dt.year\n",
    "            x_label = 'Calendar Year'\n",
    "\n",
    "        # Group the DataFrame by year and n, and calculate the sum of revenue\n",
    "        yearly_total_revenue = dailyRevenue_df.groupby('year').mean()\n",
    "\n",
    "        # Calculate the marginal increase in revenue for each extra hour of storage\n",
    "        \n",
    "        # Drop incomplete year and unnecessary columns\n",
    "        dailyRevenue_df = dailyRevenue_df.drop('year', axis=1)\n",
    "        yearly_total_revenue.drop(yearly_total_revenue.tail(1).index,inplace=True)\n",
    "\n",
    "        yearly_total_revenue.tail(years).plot.bar(figsize=(10, 6))\n",
    "        y_label = 'Average Daily Profit (A$/MW)' #? $AU / kW-year\n",
    " \n",
    "        # Set plot labels and title\n",
    "        plt_title = 'Value of Potential Storage Profit for ' + df_name\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(plt_title)\n",
    "        plt.legend(title='Hours of Storage', ncol=2)\n",
    "        plt.grid(True)\n",
    "        plt.show()  \n",
    "        \n",
    "        globals()[df_name + '_YtP'] = yearly_total_revenue\n",
    "      # globals()[df_name + '_MIR'] = marginal_revenue_increase\n",
    "    \n",
    "    \n",
    "def revenueDurationPlot(regions):\n",
    "    \"\"\"\n",
    "    Plot duration of revenue for all states and all values of n\n",
    "    https://blog.finxter.com/plotting-a-load-duration-curve-with-python/\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the DataFrame by the loads, in descending order of magnitude\n",
    "    years = range(2000, 2023)\n",
    "    dict_of_df = {}  # initialize empty dictionary\n",
    "    ns = range(2, 25, 2)\n",
    "\n",
    "    for n in ns:\n",
    "        storage_time = 'hours=' + str(n)+'.0'\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(regions), figsize=(15, 5))\n",
    "\n",
    "        for i, df_name in enumerate(regions):\n",
    "            df = globals()[df_name + '_DR']\n",
    "            for year in years:\n",
    "                DR_sorted = df.loc[df.date.dt.year == year]\n",
    "                DR_sorted = DR_sorted.sort_values(by=['date'], ascending=True)\n",
    "                DR_sorted = DR_sorted.sort_values(by=[storage_time], ascending=False)\n",
    "\n",
    "                DR_sorted['duration'] = 1\n",
    "                DR_sorted['duration'] = DR_sorted['duration'].cumsum()\n",
    "\n",
    "                DR_sorted['CumRev'] = DR_sorted[storage_time].cumsum()\n",
    "                DR_sorted['CumRevNorm'] = DR_sorted['CumRev'] / DR_sorted[storage_time].sum()*100\n",
    "\n",
    "                dict_of_df[\"df_{}\".format(year)] = DR_sorted\n",
    "\n",
    "                # Plot the load duration curve (Revenue vs Percentage of time) on the corresponding subplot\n",
    "                pltTitle = df_name\n",
    "                 #Create the line plot\n",
    "                lineplot = sb.lineplot(x=\"duration\", y=\"CumRevNorm\", data=dict_of_df['df_' + str(year)], ax=axes[i])\n",
    "\n",
    "                # Set attributes using .set() on the AxesSubplot object\n",
    "                lineplot.set(title=pltTitle, xlabel='Number of Days', ylabel='Percentage of yearly Revenue (%)')\n",
    "\n",
    "                axes[i].set_ylim(0, 100)\n",
    "                axes[i].set_xlim(0, 366)\n",
    "                axes[i].set_title(pltTitle)\n",
    "                axes[i].grid(True)  # Turn on gridlines\n",
    "\n",
    "\n",
    "        plt.suptitle('Normalised Revenue Duration Curves for ' + str(int(n / 2)) + ' hours of storage')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "def revenueDurationPlot2(region, n_values):\n",
    "    \"\"\"\n",
    "    Plot duration of revenue for a single region and multiple values of n on the same graph\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the DataFrame by the loads, in descending order of magnitude\n",
    "    #years = [2000,2010,2020,2022]\n",
    "    years = [2018, 2020, 2022]#,2022]\n",
    "\n",
    "    dict_of_df = {}  # initialize an empty dictionary\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(years), figsize=(15, 5))\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        df_name = region  # Use the specified region\n",
    "        df = globals()[df_name + '_DR']\n",
    "\n",
    "        for n in n_values:\n",
    "            storage_time = 'hours=' + str(n) + '.0'\n",
    "\n",
    "            DR_sorted = df.loc[df.date.dt.year == year]\n",
    "            DR_sorted = DR_sorted.sort_values(by=['date'], ascending=True)\n",
    "            DR_sorted = DR_sorted.sort_values(by=[storage_time], ascending=False)\n",
    "\n",
    "            DR_sorted['duration'] = 1\n",
    "            DR_sorted['duration'] = DR_sorted['duration'].cumsum()\n",
    "\n",
    "            DR_sorted['CumRev'] = DR_sorted[storage_time].cumsum()\n",
    "            DR_sorted['CumRevNorm'] = DR_sorted['CumRev'] / DR_sorted[storage_time].sum() * 100\n",
    "            print(year)   \n",
    "            #print(DR_sorted['CumRevNorm'].iloc[356:366])\n",
    "            print(DR_sorted['date'].iloc[356:366])\n",
    "\n",
    "            dict_of_df[\"df_{}_{}\".format(region, n)] = DR_sorted\n",
    "\n",
    "            # Plot the load duration curve (Revenue vs Percentage of time) on the corresponding subplot\n",
    "            pltTitle = str(year)\n",
    "            # Create the line plot\n",
    "            lineplot = sb.lineplot(x=\"duration\", y=\"CumRevNorm\", data=dict_of_df['df_{}_{}'.format(region, n)], ax=axes[i], label=f'n={n} hours')\n",
    "\n",
    "        # Set attributes using .set() on the AxesSubplot object\n",
    "        axes[i].set(title=pltTitle, xlabel='Number of Days', ylabel='Percentage of yearly Revenue (%)')\n",
    "\n",
    "        axes[i].set_ylim(0, 100)\n",
    "        axes[i].set_xlim(0, 366)\n",
    "        axes[i].set_title(pltTitle)\n",
    "        axes[i].grid(True)  # Turn on gridlines\n",
    "\n",
    "    plt.suptitle('Normalized Revenue Duration Curves for the NEM')# + region)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def marginalBenefitPlot(regions):\n",
    "    \"\"\"\n",
    "    Plot\n",
    "    \"\"\"\n",
    "    for i, df_name in enumerate(regions):\n",
    "        name = df_name + '_YtR'\n",
    "        df = globals()[name]\n",
    "        normalized_df = df.copy()\n",
    "        # Calculate the normalization factor as the rightmost column\n",
    "        #CHANGE:::::: tHIS ASSUMES RH column has max value\n",
    "        normalization_factor = normalized_df.max(axis=1)\n",
    "\n",
    "        # Divide each column by the normalization factor to normalize the data\n",
    "        pcTotal_df = normalized_df.divide(normalization_factor, axis=0) * 100\n",
    "        #print(pcTotal_df.tail(5).mean().round(2))\n",
    "        A = pcTotal_df.tail(5).mean().round(2)\n",
    "        print(A.iloc[:])\n",
    "\n",
    "        #iloc[:, 0]\n",
    "\n",
    "        # Plot each year as a different color\n",
    "        pcTotal_df.tail(10).T.plot.bar(figsize=(10, 6))\n",
    "        #print(pcTotal_df)\n",
    "        # Set plot labels and title\n",
    "        plt.xlabel('Hours of Storage')\n",
    "        plt.ylabel('Percentage of Maximumue Captured (%)')\n",
    "        pltTitle = df_name + ' Yearly Revenue Capture Percentage'\n",
    "        plt.title(pltTitle)\n",
    "        \n",
    "\n",
    "        # Display the legend\n",
    "        plt.legend(title='Year')\n",
    "        plt.ylim(0,100)\n",
    "        plt.xlim(0,12)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "def marginalBenefitPlot2(regions_dataframes):\n",
    "    \"\"\"\n",
    "    Plot the marginal benefit for each region based on the given DataFrames.\n",
    "    \"\"\"\n",
    "    for df_name, df in regions_dataframes.items():\n",
    "        normalized_df = df.copy()\n",
    "        # Calculate the normalization factor as the maximum value across all columns\n",
    "        normalization_factor = normalized_df.max().max()\n",
    "\n",
    "        # Divide each column by the normalization factor to normalize the data\n",
    "        pcTotal_df = normalized_df.divide(normalization_factor) * 100\n",
    "\n",
    "        # Plot each year as a different color\n",
    "        pcTotal_df.tail(10).T.plot.bar(figsize=(10, 6))\n",
    "\n",
    "        # Set plot labels and title\n",
    "        plt.xlabel('Hours of Storage')\n",
    "        plt.ylabel('Percentage of Maximum Value Captured')\n",
    "        pltTitle = df_name + ' Arbitrage Value of Storage as a Percentage of Maximum'\n",
    "        plt.title(pltTitle)\n",
    "\n",
    "        # Display the legend\n",
    "        plt.legend(title='Year')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlim(0, 12)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "def calculate_financial_year(row):\n",
    "    start_year = row['date'].year\n",
    "    end_year = start_year + 1\n",
    "    return f'{start_year}-{end_year}'\n",
    "\n",
    "\n",
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Autumn'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Winter'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Spring'\n",
    "    else:\n",
    "        return 'Summer'\n",
    "    \n",
    "def seasonalPlots(regions):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for df_name in regions:\n",
    "        name = df_name + '_DR'\n",
    "        df = globals()[name]\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        # Extract year and month for creating the season column\n",
    "        #df['year'] = df.index.year\n",
    "        #df['month'] = df.index.month\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        \n",
    "        \n",
    "        # Create the 'season' column using the get_season function\n",
    "        df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "        # Group by year and season, then calculate the sum of revenues\n",
    "        seasonal_revenues = df.groupby(['year', 'season'])['hours=6.0'].sum().reset_index()\n",
    "\n",
    "        seasonal_revenues.tail(200).plot(figsize=(10, 6))\n",
    "        \n",
    "        \n",
    "def diurnal_slope(regions, length, hours, a, timeframe, MA):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for df_name in regions:\n",
    "        name = df_name\n",
    "        df = globals()[name]\n",
    "        df_DR = globals()[name + '_DR']\n",
    "        \n",
    "        df_DR.set_index('date')\n",
    "        # Assuming your DataFrame is named df and contains 'date' and 'price' columns\n",
    "        # Convert the 'date' column to datetime type\n",
    "        df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "\n",
    "        # Extract the date and time components from the 'date' column\n",
    "        df['day'] = df['SETTLEMENTDATE'].dt.date\n",
    "        # Group by 'day' and find the minimum and maximum prices for each day\n",
    "        grouped = df.groupby('day').agg(min_price=('RRP', 'min'), max_price=('RRP', 'max'))\n",
    "\n",
    "        # Calculate the time difference between minimum and maximum time for each day\n",
    "        grouped['time_diff'] = (df.groupby('day')['SETTLEMENTDATE'].max() - df.groupby('day')['SETTLEMENTDATE'].min()).dt.total_seconds() / 3600  # Convert seconds to hours\n",
    "\n",
    "        # Calculate the slope of the price change for each day\n",
    "        grouped['slope'] = (grouped['max_price'] - grouped['min_price']) / grouped['time_diff']\n",
    "        hoursStr = 'hours=' + str(hours) + '.0'\n",
    "\n",
    "        if MA == True:\n",
    "            grouped['21day MA'] = grouped['slope'].rolling(window=length).mean()\n",
    "            df_DR['21day MA'] = df_DR[hoursStr].rolling(window=length).mean()\n",
    "            pltTitle = df_name + ' ' + str(length) + ' Day Moving Average'\n",
    "        else:\n",
    "            grouped['21day MA'] = grouped['slope']\n",
    "            df_DR['21day MA'] = df_DR[hoursStr]\n",
    "            pltTitle = df_name + ' Daily Revenue and Ramp Rate'\n",
    "            \n",
    "        correlation_coefficient = np.corrcoef(grouped['21day MA'], df_DR['21day MA'])[0, 1]\n",
    "        print(f'Correlation Coefficient (r) = {correlation_coefficient:.2f}')\n",
    "\n",
    "        current_date = pd.Timestamp.now()\n",
    "        years_captured = current_date - pd.DateOffset(years=timeframe)\n",
    "        filtered_df = grouped[grouped.index >= years_captured]\n",
    "        filtered_df_DR = df_DR[df_DR['date'] >= years_captured]\n",
    "\n",
    "        #filtered_df['21day MA'].plot()\n",
    "        #df_DR['21day MA'].plot()\n",
    "        \n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('$ / KW-hour', color=color)\n",
    "        lns1 = ax1.plot(filtered_df.index, filtered_df['21day MA'], label='Daily Ramp Up Rate', color=color, alpha=a)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.grid(False)\n",
    "        ax1.axis(ymin=0,ymax=60)\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('$ / KW', color=color)  # we already handled the x-label with ax1\n",
    "        lns2 = ax2.plot(filtered_df_DR['date'], filtered_df_DR['21day MA'], label='Value of ' + str(hours) + ' hours of Storage', color=color, alpha=a)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        ax2.grid(False)\n",
    "        ax2.axis(ymin=0,ymax=5000)\n",
    "\n",
    "        lns = lns1+lns2\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        ax1.legend(lns, labs, loc=0)\n",
    "        \n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        \n",
    "        #plt.plot(filtered_df.index, filtered_df['21day MA'], label='Max Min Gradient')\n",
    "        #plt.plot(filtered_df_DR['date'], filtered_df_DR['21day MA'], label='Value of Storage')\n",
    "        \n",
    "        plt.title(pltTitle)\n",
    "        #plt.xlabel('Year')\n",
    "        #plt.ylabel('$ / kW')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "\n",
    "def valueStoragePlot(regions, storage_time):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    storage_string = str(storage_time) + '.0'\n",
    "    for df_name in regions:\n",
    "        df = globals()[df_name + '_YtR']\n",
    "        plt.plot(df.index, df[storage_string], label=df_name)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Value of ' + storage_string + ' hours of storage')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue ($ / kW-year)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def totalRevenue(regions, storage_time, years):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "       # Plot the data\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    for i in storage_time:\n",
    "        storage_string = str(i) + '.0'\n",
    "        plt.bar([df_name], df[storage_string].tail(years).sum())\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Value of ' + storage_string + ' hours of storage for the last ' + str(years) + ' years')\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Revenue ($ / kW-year)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "def totalRevenue2(regions, storage_times, years):\n",
    "      # Create a figure for the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Initialize empty lists to store revenue values and x-axis positions\n",
    "    revenue_values = []\n",
    "    x_positions = np.arange(len(regions))\n",
    "\n",
    "    # Loop through the provided regions\n",
    "    for df_name in regions:\n",
    "        # Assuming you have data frames like 'df_name_YtR' for each region\n",
    "        df = globals()[df_name + '_YtR']\n",
    "\n",
    "        # Initialize an empty list to store revenue values for this region\n",
    "        region_revenue = []\n",
    "\n",
    "        for storage_time in storage_times:\n",
    "            storage_string = str(storage_time) + '.0'\n",
    "\n",
    "            # Calculate the total revenue for the last 'years' years\n",
    "            total_revenue = df[storage_string].tail(years).sum()\n",
    "\n",
    "            # Append the total revenue to the list\n",
    "            region_revenue.append(total_revenue)\n",
    "\n",
    "        # Store the revenue values for this region\n",
    "        revenue_values.append(region_revenue)\n",
    "\n",
    "    # Plot the data for each region\n",
    "    bar_width = 0.2  # Adjust the width of the bars as needed\n",
    "    for i, region in enumerate(regions):\n",
    "        plt.bar(x_positions + (i * bar_width), revenue_values[i], bar_width, label=region)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Total Revenue for {years} Years of Storage')\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Revenue ($ / kW-year)')\n",
    "    plt.xticks(x_positions + ((len(storage_times) - 1) * bar_width) / 2, regions)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def openNEM(regions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    # Select Region\n",
    "    for df_name in regions:\n",
    "        file_path = \"OpenNEM/\" + df_name + \".csv\"\n",
    "            \n",
    "        print(f'{df_name} Calling OpenNEM from csv.')\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        dataframes_dict[df_name] = df\n",
    "        \n",
    "        globals()[\"on_\" + df_name] = pd.read_csv(file_path)\n",
    "\n",
    "    return on_TAS, on_SA, on_QLD, on_NSW, on_VIC, on_AUS\n",
    "\n",
    "\n",
    "def peakTimesPlot(regions_aus, year_list):\n",
    "        \n",
    "    for df_name in regions_aus:\n",
    "        df = globals()[df_name]\n",
    "        df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "        df['year'] = df['SETTLEMENTDATE'].dt.year\n",
    "        df['month'] = df['SETTLEMENTDATE'].dt.month\n",
    "        df['hour'] = df['SETTLEMENTDATE'].dt.hour\n",
    "        df['minute'] = df['SETTLEMENTDATE'].dt.minute\n",
    "        df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "        # Group by year and date, and find highest and lowest prices\n",
    "        daily_high = df.groupby(['year', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmax()\n",
    "        daily_low = df.groupby(['year', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmin()\n",
    "        \n",
    "        # Count occurrences of each time for highest and lowest prices for each year\n",
    "        high_time_counts = df.loc[daily_high].groupby(['year', 'hour', 'minute']).size().reset_index(name='count')\n",
    "        low_time_counts = df.loc[daily_low].groupby(['year', 'hour', 'minute']).size().reset_index(name='count')\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list: # df['year'].unique():\n",
    "            high_year_data = high_time_counts[high_time_counts['year'] == year]\n",
    "            #print(year)\n",
    "            #print(high_year_data.describe())\n",
    "            plt.plot(high_year_data['hour'], high_year_data['count'], label=f'{year}', marker='o')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Highest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        #plt.xlim(0,23)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list: # df['year'].unique():\n",
    "            low_year_data = low_time_counts[low_time_counts['year'] == year]\n",
    "            #print(low_year_data.describe())\n",
    "            plt.plot(low_year_data['hour'], low_year_data['count'], label=f'{year}', marker='x')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Lowest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        #plt.xlim(0,23)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "def peakTimesPlot2(regions_aus, year_list):\n",
    "    \n",
    "    for df_name in regions_aus:\n",
    "        df = globals()[df_name]\n",
    "        df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "        df['year'] = df['SETTLEMENTDATE'].dt.year\n",
    "        df['month'] = df['SETTLEMENTDATE'].dt.month\n",
    "        df['hour'] = df['SETTLEMENTDATE'].dt.hour\n",
    "        df['minute'] = df['SETTLEMENTDATE'].dt.minute\n",
    "        df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "        # Group by year and date, and find highest and lowest prices\n",
    "        daily_high = df.groupby(['year', 'season', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmax()\n",
    "        daily_low = df.groupby(['year', 'season', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmin()\n",
    "\n",
    "        # Count occurrences of each time for highest and lowest prices for each year\n",
    "        high_time_counts = df.loc[daily_high].groupby(['year', 'hour', 'minute', 'season']).size().reset_index(name='count')\n",
    "        low_time_counts = df.loc[daily_low].groupby(['year', 'hour', 'minute', 'season']).size().reset_index(name='count')\n",
    "        print(low_time_counts)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list: # df['year'].unique():\n",
    "            high_year_data = high_time_counts[high_time_counts['year'] == year]\n",
    "            plt.plot(high_year_data['hour'], high_year_data['count'], label=f'{year}', marker='o')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Highest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        #plt.xlim(0,23)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list: # df['year'].unique():\n",
    "            low_year_data = low_time_counts[low_time_counts['year'] == year]\n",
    "            plt.plot(low_year_data['hour'], low_year_data['count'], label=f'{year}', marker='x')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Lowest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        #plt.xlim(0,23)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "def peakTimesPlot3(regions_aus, year_list):\n",
    "    \n",
    "    for df_name in regions_aus:\n",
    "        df = globals()[df_name]\n",
    "        df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "        df['year'] = df['SETTLEMENTDATE'].dt.year\n",
    "        df['month'] = df['SETTLEMENTDATE'].dt.month\n",
    "        df['hour'] = df['SETTLEMENTDATE'].dt.hour\n",
    "        df['minute'] = df['SETTLEMENTDATE'].dt.minute\n",
    "        df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "        # Group by year, season, and date, and find highest and lowest prices\n",
    "        daily_high = df.groupby(['year', 'season', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmax()\n",
    "        daily_low = df.groupby(['year', 'season', df['SETTLEMENTDATE'].dt.date])['RRP'].idxmin()\n",
    "\n",
    "        # Count occurrences of each time for highest and lowest prices for each year and season\n",
    "        high_time_counts = df.loc[daily_high].groupby(['year', 'season', 'hour', 'minute']).size().reset_index(name='count')\n",
    "        low_time_counts = df.loc[daily_low].groupby(['year', 'season', 'hour', 'minute']).size().reset_index(name='count')\n",
    "        \n",
    "        seasons = df['season'].unique()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list:\n",
    "            for season in seasons:\n",
    "                high_year_season_data = high_time_counts[(high_time_counts['year'] == year) & (high_time_counts['season'] == season)]\n",
    "                if not high_year_season_data.empty:\n",
    "                    label = f'{year} - {season}'\n",
    "                    plt.plot(high_year_season_data['hour'], high_year_season_data['count'], label=label, marker='o')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Highest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in year_list:\n",
    "            for season in seasons:\n",
    "                low_year_season_data = low_time_counts[(low_time_counts['year'] == year) & (low_time_counts['season'] == season)]\n",
    "                if not low_year_season_data.empty:\n",
    "                    label = f'{year} - {season}'\n",
    "                    plt.plot(low_year_season_data['hour'], low_year_season_data['count'], label=label, marker='x')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Number of Occurrences')\n",
    "        plt.title('Timing of Daily Lowest Prices in ' + df_name)\n",
    "        plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return low_time_counts, high_time_counts\n",
    "        \n",
    "def ValuePlotMA(regions, length, hours):\n",
    "    for df_name in regions:\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "        df = globals()[df_name + '_DR']\n",
    "        hoursStr = 'hours=' + str(hours) + '.0'\n",
    "\n",
    "        df['MA'] = df[hoursStr].rolling(window=length).mean()\n",
    "\n",
    "        plt.plot(df['date'], df['MA'], label=df_name)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(df_name + ' ' + str(length) + ' Day Moving Average Value of ' + str(hours) + ' hours of storage')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Revenue ($ / kW)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which='both')\n",
    "        plt.minorticks_on()\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest and Lowest Price graph \n",
    "def extraStuff():\n",
    "    df = VIC\n",
    "    df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
    "    df['hour'] = df['SETTLEMENTDATE'].dt.hour\n",
    "    df['minute'] = df['SETTLEMENTDATE'].dt.minute\n",
    "\n",
    "    # Group by date and find highest and lowest prices\n",
    "    daily_high = df.groupby(df['SETTLEMENTDATE'].dt.date)['RRP'].idxmax()\n",
    "    daily_low = df.groupby(df['SETTLEMENTDATE'].dt.date)['RRP'].idxmin()\n",
    "\n",
    "    # Count occurrences of each time for highest and lowest prices\n",
    "    high_time_counts = df.loc[daily_high].groupby(['hour', 'minute']).size().reset_index(name='count')\n",
    "    low_time_counts = df.loc[daily_low].groupby(['hour', 'minute']).size().reset_index(name='count')\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(high_time_counts['hour'], high_time_counts['count'], label='Highest Price', marker='o')\n",
    "    plt.plot(low_time_counts['hour'], low_time_counts['count'], label='Lowest Price', marker='x')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Number of Occurrences')\n",
    "    plt.title('Occurrences of Highest and Lowest Prices by Hour (Victoria)')\n",
    "    plt.xticks(range(24))  # Set x-axis to show all hours\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    ###\n",
    "    #Calculate the number of days from the earliest date\n",
    "\n",
    "    VIC_DR['days_passed'] = (VIC_DR['date'] - VIC_DR['date'].min()).dt.days\n",
    "\n",
    "    # Define an interest rate (example: 5% annually)\n",
    "    interest_rate = 0.05\n",
    "\n",
    "    # Calculate present value using compound interest formula: PV = FV / (1 + r)^n\n",
    "    VIC_DR['present_value'] = VIC_DR['hours=6.0'] / (1 - interest_rate) ** (VIC_DR['days_passed'] / 365)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "\n",
    "    # Plot DataFrame 2\n",
    "    plt.plot(VIC_DR['date'], VIC_DR['present_value'], label='VIC Present Value (5%)')\n",
    "    plt.plot(VIC_DR['date'], VIC_DR['hours=6.0'], label='VIC Revenue')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('180 Day Moving Average Diurnal Price Differential')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue ($ / kW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='minor')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    ###\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    on_VIC['date'] = pd.to_datetime(on_VIC['date'])\n",
    "\n",
    "    # Extract only the month part from the dates\n",
    "    on_VIC['month'] = on_VIC['date'].dt.month\n",
    "\n",
    "    # Plot the Diurnal Price Differential from VIC_aMR\n",
    "    plt.plot(VIC_aMR['month'], VIC_aMR['hours=6.0'], label='Avg. Monthly Revenue')\n",
    "\n",
    "    # Plot the Renewables Percentage from on_VIC\n",
    "    sb.lineplot(x=\"month\", y=\"Renewables — %\", data=on_VIC, label='Renewables %')\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(VIC_aMR['month'], VIC_aMR['hours=6.0'], label='Avg. Monthly Revenue', color='tab:blue')\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Revenue ($ / kW)', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Create a second y-axis (right side)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax2.plot(on_VIC['month'], on_VIC['Renewables — %'], label='%R', color='tab:orange')\n",
    "    ax2.set_ylabel('% Renewables', color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('180 Day Moving Average Diurnal Price Differential and Renewables')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
